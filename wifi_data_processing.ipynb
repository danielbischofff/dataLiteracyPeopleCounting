{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-09T10:05:57.965481400Z",
     "start_time": "2024-01-09T10:05:53.635325500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Wireshark is installed, but cannot read manuf !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scapy.all import PcapReader\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scapy.layers.dot11 import Dot11"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this jupyter notebook we want to read out our recorded data and then use the data to produce various plots.\n",
    "\n",
    "To start, we first need to write a function that can read out our data.\n",
    "Here we decided to directly readout the data as it is, but we will probably use csv files instead. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5928335b1d2a8b9b"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def read_pcapng(file_path):\n",
    "    \"\"\"\n",
    "    This function reads the wireshark packets directly and stores them in a pandas data frame\n",
    "    :param file_path: This is the path to the to be read file\n",
    "    :return: Pandas dataframe with the read out data\n",
    "    \"\"\"\n",
    "    timestamps, senders, receivers, signal_strengths = [], [], [], []\n",
    "    \n",
    "    for pkt in PcapReader(filepath):\n",
    "        if pkt.haslayer('Dot11'):\n",
    "            timestamp = datetime.fromtimestamp(float(pkt.time))\n",
    "            rssi = pkt.getlayer('RadioTap').dBm_AntSignal\n",
    "            sender, receiver = pkt.addr2, pkt.addr1\n",
    "    \n",
    "            timestamps.append(timestamp)\n",
    "            senders.append(sender)\n",
    "            receivers.append(receiver)\n",
    "            signal_strengths.append(rssi)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Timestamp': timestamps,\n",
    "        'Source': senders,\n",
    "        'Destination': receivers,\n",
    "        'Signal-Strength': signal_strengths\n",
    "    })\n",
    "    \n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T11:36:13.596797100Z",
     "start_time": "2024-01-09T11:36:13.580553900Z"
    }
   },
   "id": "4ee7211e0d122302"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following cell, we are using the function, to read out one file.\n",
    "To find this file, we have to give the path were it is located."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c8586be2db28a5a"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Timestamp             Source        Destination  \\\n",
      "0    2023-12-12 08:26:31.261669  ea:f5:71:1e:46:bc  00:f6:63:ad:8a:a0   \n",
      "1    2023-12-12 08:26:31.294627  ea:f5:71:1e:46:bc  00:f6:63:ad:8a:a0   \n",
      "2    2023-12-12 08:26:31.879087  ea:f5:71:1e:46:bc  00:f6:63:ad:8a:a0   \n",
      "3    2023-12-12 08:26:31.897903               None  ea:f5:71:1e:46:bc   \n",
      "4    2023-12-12 08:26:32.782836  3a:cf:d1:b9:84:d6  ff:ff:ff:ff:ff:ff   \n",
      "...                         ...                ...                ...   \n",
      "2425 2023-12-12 08:36:23.525919               None  ac:bd:70:d8:71:cc   \n",
      "2426 2023-12-12 08:36:25.542764               None  26:3b:22:93:ac:e6   \n",
      "2427 2023-12-12 08:36:26.894979  16:cf:27:01:cb:7b  ff:ff:ff:ff:ff:ff   \n",
      "2428 2023-12-12 08:36:26.975547  16:cf:27:01:cb:7b  ff:ff:ff:ff:ff:ff   \n",
      "2429 2023-12-12 08:36:26.995736  16:cf:27:01:cb:7b  ff:ff:ff:ff:ff:ff   \n",
      "\n",
      "      Signal-Strength  \n",
      "0                 -81  \n",
      "1                 -83  \n",
      "2                 -84  \n",
      "3                 -63  \n",
      "4                 -70  \n",
      "...               ...  \n",
      "2425              -82  \n",
      "2426              -61  \n",
      "2427              -80  \n",
      "2428              -75  \n",
      "2429              -74  \n",
      "\n",
      "[2430 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filepath = os.getcwd()\n",
    "# print(filepath)\n",
    "filepath += \"\\\\Lecture-Data\\\\Lecture-12-12_00002_20231212082631.pcapng\" # Lecture data\n",
    "# filepath += \"\\\\lecture-data-090124\\\\lecture090124_00001_20240109103111.pcapng\"\n",
    "# filepath += \"\\\\lecture-data-090124\\\\lecture-data-090124.csv\"\n",
    "\n",
    "df = read_pcapng(filepath)\n",
    "# df = pd.read_csv(filepath)\n",
    "\n",
    "print(df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T11:20:20.826691100Z",
     "start_time": "2024-01-09T11:20:18.649779100Z"
    }
   },
   "id": "c1b9863ed88f8a74"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we want to know how many unique addresses there are. \n",
    "For that we first merge all Source and Destination addresses together, so that we can see all possible mac addresses.\n",
    "Additionally, we want to remove all None values from our data. \n",
    "\n",
    "Then we can use the pandas dataframe to get the count of all unique values. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c25ca55c32f4ec13"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n"
     ]
    }
   ],
   "source": [
    "addresses = pd.concat((df['Source'].astype(str), df['Destination'].astype(str)))\n",
    "addresses = addresses.drop(addresses[addresses == 'None'].index)\n",
    "\n",
    "print(addresses.nunique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T11:20:20.842318100Z",
     "start_time": "2024-01-09T11:20:20.826691100Z"
    }
   },
   "id": "cc6f6a0182a2dc49"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To see how much noise we have in the data, we are then going to count how many times a unique address is showing up. \n",
    "For that, we loop over all unique addresses and store the number of occurrences in a new np array.\n",
    "The number of occurrences is mapped to the unique addresses on an index basis. To match the address to the count, we need to match it to the index it has in unique_addresses."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "279f288d246358e4"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.000e+00 2.000e+00 2.000e+00 1.000e+00 6.000e+00 2.000e+00 3.000e+00\n",
      " 2.000e+00 2.000e+00 1.000e+00 2.000e+00 9.400e+01 2.000e+00 1.000e+00\n",
      " 3.000e+00 1.000e+00 2.000e+00 2.000e+00 2.000e+00 2.000e+00 1.000e+00\n",
      " 1.200e+01 1.000e+00 2.000e+00 1.000e+00 1.000e+00 1.000e+00 2.000e+00\n",
      " 1.000e+00 1.000e+00 2.000e+00 1.000e+00 2.000e+00 1.200e+01 2.000e+00\n",
      " 1.000e+00 1.000e+00 2.000e+00 1.000e+01 1.000e+01 1.000e+00 1.000e+00\n",
      " 2.000e+00 2.000e+00 1.000e+00 1.000e+00 2.000e+00 4.000e+00 1.000e+00\n",
      " 2.000e+00 2.000e+00 6.000e+00 2.000e+00 2.000e+00 1.000e+00 8.000e+00\n",
      " 4.000e+00 1.000e+00 2.000e+00 1.000e+00 2.000e+00 2.000e+00 4.000e+00\n",
      " 4.000e+00 7.000e+00 1.000e+00 7.000e+00 2.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 7.400e+01 3.000e+00 3.000e+00 4.000e+00 2.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 3.000e+00 6.400e+01 3.000e+00 1.000e+00 2.000e+00\n",
      " 1.000e+00 2.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 2.000e+00 1.000e+00 1.420e+02 1.400e+01 1.000e+00 1.000e+00\n",
      " 2.000e+00 1.000e+00 1.000e+00 2.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 2.000e+00 3.000e+00 2.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 8.000e+00 9.000e+00 1.000e+00 2.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 3.000e+00 1.000e+00 2.000e+00\n",
      " 1.000e+00 2.000e+00 2.000e+00 1.000e+00 2.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 2.000e+00 2.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+01\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+01\n",
      " 2.000e+00 2.000e+00 2.000e+00 1.000e+00 2.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 2.000e+00 6.000e+01 1.000e+00 1.000e+00\n",
      " 3.000e+00 2.000e+00 1.000e+00 1.000e+00 1.000e+00 2.000e+00 3.000e+00\n",
      " 1.500e+01 1.000e+00 1.000e+00 3.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 2.000e+00 2.000e+00 1.000e+00 2.000e+00 4.000e+00 1.000e+00 3.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 3.000e+00 2.000e+00\n",
      " 1.000e+00 1.000e+00 2.000e+00 1.000e+00 2.000e+00 2.000e+00 1.300e+01\n",
      " 3.000e+00 4.000e+00 3.000e+00 3.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 4.000e+00 3.000e+01 2.000e+00 3.000e+00 3.000e+00 1.000e+00 1.000e+00\n",
      " 2.000e+00 3.000e+00 1.000e+00 1.000e+00 2.900e+01 4.000e+00 1.900e+01\n",
      " 2.000e+00 1.000e+00 2.200e+01 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 3.200e+01 3.400e+01 3.000e+00 1.000e+00 1.000e+00 1.000e+00 3.000e+00\n",
      " 2.000e+00 2.000e+00 1.000e+00 2.000e+00 2.000e+00 1.000e+00 1.200e+01\n",
      " 3.000e+00 2.000e+00 8.000e+00 2.000e+00 1.000e+00 2.000e+00 2.000e+00\n",
      " 2.000e+00 1.000e+00 1.000e+00 3.000e+00 1.000e+00 1.000e+00 2.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 4.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 4.000e+00 2.000e+00 3.000e+00 1.000e+00 1.000e+00 2.000e+00 1.000e+00\n",
      " 2.000e+00 1.000e+00 1.000e+00 1.000e+00 3.000e+00 1.000e+00 2.000e+00\n",
      " 2.000e+00 1.000e+00 1.000e+00 2.000e+00 1.000e+00 2.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 2.000e+00 2.000e+00 2.000e+00 1.000e+00 2.000e+00\n",
      " 2.000e+00 1.000e+00 2.000e+00 2.000e+00 1.000e+00 2.000e+00 1.000e+00\n",
      " 1.000e+00 2.000e+00 2.000e+00 1.000e+00 2.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 4.000e+00 2.000e+00 3.000e+00 1.000e+00 4.000e+00 2.000e+00\n",
      " 1.000e+00 2.000e+00 2.000e+00 2.000e+00 2.000e+00 2.000e+00 2.000e+00\n",
      " 1.000e+00 2.000e+00 3.000e+00 1.000e+00 1.000e+00 2.000e+00 3.000e+00\n",
      " 1.000e+00 2.000e+00 2.000e+00 2.000e+00 1.000e+00 2.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 2.000e+00 2.000e+00 1.000e+00 3.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 2.000e+00 1.270e+02 3.000e+00 1.000e+00\n",
      " 2.000e+00 2.000e+00 6.800e+01 1.000e+00 1.000e+00 2.000e+00 2.000e+00\n",
      " 2.000e+00 1.000e+00 3.000e+00 1.000e+00 1.000e+00 1.100e+01 2.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 2.000e+00 1.000e+00 2.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 2.000e+00 1.000e+00 2.000e+00 2.000e+00 1.000e+00\n",
      " 1.000e+00 2.000e+00 1.000e+00 1.000e+00 3.000e+00 5.000e+00 4.000e+00\n",
      " 2.000e+00 2.000e+00 2.000e+00 1.000e+00 2.000e+00 2.000e+00 1.000e+00\n",
      " 2.000e+00 3.000e+00 1.000e+00 3.000e+00 2.000e+00 2.000e+00 1.000e+00\n",
      " 6.000e+00 9.000e+00 1.000e+00 1.000e+00 1.600e+01 3.000e+00 1.000e+00\n",
      " 4.000e+00 2.000e+00 2.000e+00 1.000e+00 1.000e+00 1.000e+00 2.000e+00\n",
      " 2.000e+00 1.000e+00 1.000e+00 2.000e+00 1.000e+00 2.000e+00 1.000e+00\n",
      " 2.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 4.000e+00 4.000e+00 2.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 2.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 4.000e+00 3.000e+00 1.000e+00 2.000e+00 1.000e+00 1.000e+00 4.000e+00\n",
      " 1.000e+00 2.000e+00 6.000e+00 1.000e+00 2.000e+00 2.000e+00 3.000e+00\n",
      " 1.044e+03 9.400e+01 1.500e+01 6.000e+00 4.000e+00 7.000e+01 4.000e+00\n",
      " 1.600e+01 1.180e+02 1.600e+01 3.000e+00 5.000e+00]\n"
     ]
    }
   ],
   "source": [
    "unique_addresses = addresses.unique()\n",
    "addresses_counts = np.zeros(addresses.nunique())\n",
    "for index, address in enumerate(unique_addresses):\n",
    "    addresses_counts[index] = addresses[addresses == address].count()\n",
    "\n",
    "print(addresses_counts)\n",
    "\n",
    "addresses_counts.sort()\n",
    "print(addresses_counts[:-1])\n",
    "filtered_addresses_count = addresses_counts[np.where(addresses_counts > 3)]\n",
    "print(filtered_addresses_count)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T11:23:18.117737900Z",
     "start_time": "2024-01-09T11:23:18.015622800Z"
    }
   },
   "id": "e012783283ecc6d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To read multiple files at once, we wrote a function, that can read multiple files at once. \n",
    "To spead up that process, we use multiprocessing, so that we can read multiple files in parallel.\n",
    "\n",
    "There is also a new itteration of the filepath, so that the user can change it here directly, so that he can quickly read out other data in other folders."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4ff16234d9a1932"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Timestamp             Source        Destination  \\\n",
      "0     2023-12-12 08:26:31.261669  ea:f5:71:1e:46:bc  00:f6:63:ad:8a:a0   \n",
      "1     2023-12-12 08:26:31.294627  ea:f5:71:1e:46:bc  00:f6:63:ad:8a:a0   \n",
      "2     2023-12-12 08:26:31.879087  ea:f5:71:1e:46:bc  00:f6:63:ad:8a:a0   \n",
      "3     2023-12-12 08:26:31.897903               None  ea:f5:71:1e:46:bc   \n",
      "4     2023-12-12 08:26:32.782836  3a:cf:d1:b9:84:d6  ff:ff:ff:ff:ff:ff   \n",
      "...                          ...                ...                ...   \n",
      "99888 2023-12-12 08:15:03.673286               None  d0:16:b4:fc:98:c3   \n",
      "99889 2023-12-12 08:15:03.802218  de:38:e5:24:8d:9d  ff:ff:ff:ff:ff:ff   \n",
      "99890 2023-12-12 08:15:05.086811  6e:36:4c:8b:83:ab  ff:ff:ff:ff:ff:ff   \n",
      "99891 2023-12-12 08:15:05.599234  fa:c6:91:03:73:0b  ff:ff:ff:ff:ff:ff   \n",
      "99892 2023-12-12 08:15:05.621162  fa:c6:91:03:73:0b  ff:ff:ff:ff:ff:ff   \n",
      "\n",
      "       Signal-Strength  \n",
      "0                  -81  \n",
      "1                  -83  \n",
      "2                  -84  \n",
      "3                  -63  \n",
      "4                  -70  \n",
      "...                ...  \n",
      "99888              -59  \n",
      "99889              -79  \n",
      "99890              -79  \n",
      "99891              -70  \n",
      "99892              -72  \n",
      "\n",
      "[99893 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "filepath = os.getcwd()\n",
    "# print(filepath)\n",
    "filepath += \"\\\\Lecture-Data\"\n",
    "\n",
    "\n",
    "def read_multiple_pcapng(directory):\n",
    "    \"\"\"\n",
    "    This function is meant to read out multiple files at once\n",
    "    :param directory: This is the path to the folder that we want to read out\n",
    "    :return: A pandas dataframe with the data of multiple files.\n",
    "    \"\"\"\n",
    "    # Define the path to the directory containing the pcapng files\n",
    "    path = os.path.join(directory, \"*.pcapng\")\n",
    "\n",
    "    # Identify all pcapng files in the directory\n",
    "    all_files = glob.glob(path)\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Map the read_single_pcapng function to all_files\n",
    "        results = list(executor.map(read_pcapng, all_files))\n",
    "\n",
    "    # Concatenate the DataFrames obtained from individual files\n",
    "    df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Call the function\n",
    "df = read_multiple_pcapng(filepath)\n",
    "\n",
    "print(df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T11:38:02.283939300Z",
     "start_time": "2024-01-09T11:36:41.149489300Z"
    }
   },
   "id": "240829de1492512b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we are counting again, how many unique addresses there are."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aac6d9b768a20c7"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12073\n",
      "11764\n",
      "842\n"
     ]
    }
   ],
   "source": [
    "addresses = pd.concat((df['Source'].astype(str), df['Destination'].astype(str)))\n",
    "\n",
    "print(addresses.nunique())\n",
    "print(df['Source'].nunique())\n",
    "print(df['Destination'].nunique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T11:38:51.882218Z",
     "start_time": "2024-01-09T11:38:51.712006100Z"
    }
   },
   "id": "7ff1615ab71cbb47"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To filter out noise again, we want to find out how many times a unique address shows up.\n",
    "\n",
    "To speed up the process, we first convert the pandas dataframe to a numpy array. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7b22577a6112ed7"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "unique_addresses = addresses.unique()\n",
    "addresses_counts = np.zeros((addresses.nunique(), 2))\n",
    "unique_addresses_count = addresses.nunique()\n",
    "addresses = addresses.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T12:21:46.023303Z",
     "start_time": "2024-01-09T12:21:45.901869100Z"
    }
   },
   "id": "11f787f922155baf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To see if there are addresses, that show up during the whole data set, as a sanity check, we are going to iterate over all unique addresses and give out the first and last index of the address. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cb3574221bfb93b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12073/12073 [01:13<00:00, 163.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.98946e+05 0.00000e+00]\n",
      " [9.98880e+04 3.00000e+00]\n",
      " [5.00000e+00 4.00000e+00]\n",
      " ...\n",
      " [1.98525e+05 1.98496e+05]\n",
      " [1.99269e+05 1.99227e+05]\n",
      " [1.99324e+05 1.99324e+05]]\n"
     ]
    }
   ],
   "source": [
    "for index, address in tqdm(enumerate(unique_addresses), total=unique_addresses_count):\n",
    "    indices = np.where(addresses == address)\n",
    "    addresses_counts[index, 1] = indices[0][0]\n",
    "    addresses_counts[index, 0] = indices[0][-1]\n",
    "\n",
    "print(addresses_counts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T09:08:07.578545900Z",
     "start_time": "2024-01-09T09:06:53.568728500Z"
    }
   },
   "id": "3935148e521b492c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we want to find out how many times a unique address shows up."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "460a7e11b5c817e9"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12073/12073 [01:10<00:00, 170.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.3900e+02 1.3900e+02]\n",
      " [4.9183e+04 4.9183e+04]\n",
      " [2.0000e+00 2.0000e+00]\n",
      " ...\n",
      " [5.0000e+00 5.0000e+00]\n",
      " [8.0000e+00 8.0000e+00]\n",
      " [1.0000e+00 1.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "for index, address in tqdm(enumerate(unique_addresses), total=unique_addresses_count):\n",
    "    addresses_counts[index] = len(np.where(addresses == address)[0])\n",
    "\n",
    "print(addresses_counts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T09:11:34.541873500Z",
     "start_time": "2024-01-09T09:10:23.670845300Z"
    }
   },
   "id": "3829d32acaffa794"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we are calculating the mean occurrence of all addresses. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1c9274e3c9c422c"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.54816533 16.54816533]\n"
     ]
    }
   ],
   "source": [
    "print(addresses_counts.mean(axis=0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-09T09:12:37.202007100Z",
     "start_time": "2024-01-09T09:12:37.183574Z"
    }
   },
   "id": "9baed0ab9e770a88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "936c6ee061fbe9cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
